%!TeX root = ../main.Rnw

\section{Experimental results}\label{sec:results}
The proposed algorithms are tested and compared against the complete algorithm
OD+ID and the decentralized algorithm DiMPP. We compare them on a large set of
randomly generated problem instances. Each instance in the set consists of a
$16 \times 16$ 8-connected grid. Each grid cell has a $20\%$ chance of
containing an impassable obstacle as specified in \autoref{sec:problem}. Agents
are placed randomly in the grid such that no two agents have the same starting
position. Each agent is also given a randomly chosen destination location.
These are also picked in such a way that no two agents have the same
destination. Note that the starting position of one agent might be the
destination of another. There is a time limit of 2000ms to solve each problem
instance. This limit is arbitrary but it was picked to show representative
results. All experiments were implemented in Python 3.6 and ran
on a single core of an AMD FX-8120 clocked at 3.1GHz. Our main hypothesis is
that our proposed algorithms are able to solve more problem instances and are
able to find solutions faster than OD+ID is. We expect that OD+ID finds higher
quality paths. We expect that our proposed methods perform on an equal or
slightly worse level than DiMPP on all aspects.

Before being able to evaluate DPCA* we have to determine the weights that are
used during voting. To do this we used simulated annealing
\citep{kirkpatrick1983} with an initial temperature of 100 which drops to 1 in
unit steps. Each iteration of the simulated annealing process consists of 200
problem instances which contain between 2 and 50 agents. After each drop of
temperature we generate a new set of 200 problem instances to prevent
overfitting. The results of annealing are shown in \autoref{tbl:annealing}. The
path length weight measures the increase of path length; the conflicts solved
weight measures how many additional conflicts were solved by a proposal; and
the partial solution weight is used as a penalty so that proposals which solve
an instance by assigning minimal explicit priorities are preferred.

\begin{table}[t]
    \centering
    \caption{Evaluation weights as determined by simulated annealing. Empty
        fields mean that the weight is not used by that algorithm.}
    \label{tbl:annealing}
    \begin{tabularx}{\columnwidth}{l|r|r|X}
        & Path length & Conflicts solved & Partial solution \\ \hline
        \Sexpr{cannonical.names[3]} & 4.744 & 5.291 &  \\
        \Sexpr{cannonical.names[4]} & 0.312 & 5.570 & 2.677
    \end{tabularx}
\end{table}

<<perfgraph,echo=F,warning=F,cache=T,cache.beater=dat,dev='tikz',fig.height=2,fig.width=5,fig.align='center',fig.pos='t',fig.cap=perfgraph.title>>=
op <- par(mar = c(3.8, 3.8, 1, 1))
plot(c(),
    type='l',
    log='y',
    col='red',
    xlim=c(0,10000),
    ylim=c(5, 2000),
    xlab='Problem instance',
    ylab='Time (ms)',
    frame.plot=F
)
lines(seq(length(od)), od, col='red')
lines(seq(length(naive)), naive, col='blue')
lines(seq(length(base)),  base,  col='green')
lines(seq(length(plus)),  plus,  col='grey')
lines(seq(length(dimpp)), dimpp, col='maroon')
legend("bottomright",
       legend=cannonical.names[c(1:4,8)],
       col=color.set[c(1:4,8)],
       pch=16,
       bty="n",
       cex=.8)
@

To compare the algorithms we give them a set of 10000 problem instances which
have between 2 and \agentsupb agents. When we record the time required to solve
each instance and sort them in ascending order we can plot them in a
\emph{performance graph} as shown in \autoref{fig:perfgraph}. The $x$-axis
shows the index of the sorted instance. It is not necessarily the case that the
$n$th instance for one algorithm is the same as the $n$th instance for a
different algorithm. The $y$-axis shows the time it takes an algorithm to solve
that instance. A lower line in the performance graph means that the algorithm
was able to solve instances faster. Instances that were not solved within the
2000ms time limit are not plotted so the performance graph also indicates how
many instances were successfully solved by an algorithm. When a plot which
extends further along the $x$-axis it means that the algorithm was able to
solve more instances. \autoref{fig:perfgraph} shows that PCA* is generally
slower than OD+ID. The latter was able to solve $\Sexpr{round(naive.vs.odid *
100, 1)}\%$ of the instances that both algorithms were able to solve faster.
DPCA* and DPCA*+ have a similar performance. DPCA* is slightly faster on each
individual instance by $\Sexpr{signif(abs(base.vs.plus$estimate * 1000), 2)}$
ms as shown by a one-sided paired t-test $(t(\Sexpr{base.vs.plus$parameter}) =
\Sexpr{round(base.vs.plus$statistic, 2)}, p=\Sexpr{signif(base.vs.plus$p.value,
3)})$. Both algorithms perform similar to DiMPP until 2500 instances. After
that point DiMPP is slower.

<<solved,echo=F,cache=T,cache.beater=solved.aggr,dev='tikz',fig.width=5,fig.height=2,fig.align='center',fig.pos='t',fig.cap=solved.title>>=
op <- par(mar = c(4, 3.8, 1, 1))
plot(1,
    type='l',
    xlim=c(0,40),
    ylim=c(0,1),
    xlab='Agents in problem instance',
    ylab='Percentage solved',
    frame.plot = F,
    yaxs="i", xaxs="i"
)
for(i in c(1:4,8)) {
    lines(solved.aggr[solved.aggr[,1]==algorithms[i],]$time,
        col=color.set[i])
}
legend("bottomleft",
       legend=cannonical.names[c(1:4,8)],
       col=color.set[c(1:4,8)],
       pch=16,
       bty="n",
       cex=.8)
@

<<quality,eval=F,echo=F,cache=T,cache.beater=solved,results='asis'>>=
quality.table = t(rbind(paste(round(solved$time * 100, 1), "%"),
                        round(lengths$length, 2)))
colnames(quality.table) = c('Instances solved', 'Length')
rownames(quality.table) = cannonical.names
xtable(quality.table[c(1:4,8),],
       label='tbl:quality',
       caption='Solution quality of algorithms. Length is the sum of the lengths
       of the paths for a single problem instance.',
       align='l|r|r',
       digits=2
)
@

<<lengths,echo=F,warning=F,cache=T,cache.beater=dat.lengths,dev='tikz',fig.width=5,fig.height=2,fig.align='center',fig.pos='t',fig.cap=lengths.title>>=
op <- par(mar = c(3.8, 3.8, 1, 1))
plot(c(),
     type='n',
     xlim=c(5,40),
     ylim=c(100,360),
     frame.plot=F,
     xaxs="i", yaxs="i",
     xlab='Agents in problem instance',
     ylab='Sum of path lengths',
     yaxs="i",
     xaxs="i"
)
for (i in c(1:4,8))
{
    lines(dat.lengths[dat.lengths[,2]==algorithms[i],]$length, col=color.set[i])
}
legend("topleft",
       legend=cannonical.names[c(1:4,8)],
       col=color.set[c(1:4,8)],
       pch=16,
       cex=.8,
       bty='n')
@

\autoref{fig:solved} shows how many instances each algorithm can solve within
the time limit for varying number of agents in an instance. It shows a
similar picture as \autoref{fig:perfgraph} where OD+ID and PCA* solved only few
instances while DPCA* and DPCA*+ are able to solve problems that include more
agents. DiMPP falls in the between of these two groups. \autoref{fig:lengths} 
shows how long the paths found by each algorithm are. In general DPCA* and 
DPCA*+ find longer paths than OD+ID. DiMPP causes the largest detours of all 
tested algorithms. PCA* finds the shortest paths of all algorithms.

<<dialogues,echo=F,cache=T,cache.beater=conflicts.aggr,dev='tikz',fig.width=5,fig.height=2,fig.align='center',fig.pos='t',fig.cap=conflicts.title>>=
op <- par(mar = c(4, 3.8, 1, 1))
plot(1,
    type='l',
    xlim=c(0,40),
    ylim=c(0,35),
    xlab='Number of agents',
    ylab='Dialogues',
    frame.plot = F,
    yaxs="i", xaxs="i"
)
for(i in c(2:4))
{
    lines(conflicts.aggr[conflicts.aggr[,1]==algorithms[i],]$solved.conflicts,
      col=color.set[i])
}
legend("topleft", legend=cannonical.names[2:4], col=color.set[2:4], pch=16,
    bty="n", cex=.8)
@

The average number of dialogues that have taken place to find a solution is
shown in \autoref{fig:dialogues}. It shows that more dialogues are needed when
the problem instance contains more agents. DPCA* and DPCA*+ need a similar
amount of dialogues, and they need a large amount more than PCA*.
again, One thing that is noticeable is that the number of dialogues required
rises exponentially when the number of agents increases, but at some point this
trend trails off. This point lies around 15 agents for PCA* and around 25
agents for DPCA* and DPCA*+.

<<conflict-sizes,echo=F,cache=T,cache.beater=sizes.aggr,dev='tikz',fig.width=5,fig.height=2,fig.align='center',fig.pos='t',fig.cap=sizes.title>>=
op <- par(mar = c(4, 3.8, 1, 1))
plot(1,
    type='n',
    xlim=c(1,40),
    ylim=c(0,15),
    xlab="Agents in problem",
    ylab="Agents in conflicts",
    xaxs="i",
    yaxs="i",
    frame.plot = F
)
points(sizes.aggr$X2[1:31], col='red', pch=8)
points(sizes.aggr$X3, col='blue', pch=4)
points(sizes.aggr$X4, col='green', pch=3)
lines(smooth.spline(sizes.aggr$X2[1:31], spar=0.35), col="red")
lines(smooth.spline(sizes.aggr$X3[1:31], spar=0.35), col="blue")
lines(smooth.spline(sizes.aggr$X4, spar=0.35), col="green")
legend("topleft",
       legend=c('2 agents', '3 agents', '4 agents'),
       col=c("red", "blue", "green"),
       pch=c(8, 4, 3),
       bty='n')
@

There have been many similarities between DPCA* and DPCA*+ in the above graphs.
\autoref{fig:conflict-sizes} shows how many agents participate in most DPCA*+
dialogues. It shows that most dialogues involve just two agents. There are on
average $\Sexpr{signif(max(sizes.aggr$X3), 3)}$ dialogues per instance with
three agents involved, and only $\Sexpr{signif(max(sizes.aggr$X4), 3)}$ with
four agents involved. DPCA*+ often does not have to solve more complex
dialogues than DPCA* but it does use a more complex strategy and thus it is
slightly slower.
