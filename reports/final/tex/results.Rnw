%!TeX root = ../main.Rnw

\section{Experimental results}\label{sec:results}
% TODO: investigate run time vs number of conflicts found

The three proposed algorithms are tested and compared against the complete
algorithm OD+ID. To do this we compare them on a number of problem instances.
Each instance consists of a $16 \times 16$ 8-connected grid. Each grid cell has
a $20\%$ chance of containing an impassable obstacle. Agents cannot enter these
grid locations, but the obstacles do not block agents from moving along
diagonals as specified in \autoref{sec:problem}. Agents are placed randomly in
the grid such that no two agents have the same starting position, each agent is
also given a randomly chosen destination location, again these are picked in
such a way that no two agents have the same destination. Note that the starting
position of one agent might be the destination of another.

DPCA* and WDPCA*  both need to evaluate proposals
made by agents. The proposals are evaluated on several different effects that
they have on an agent's plan. To be able to evaluate a proposal properly there
are several weights that need to be set so that each effect is taken into
account as much as it needs to be. To set the weights simulated annealing
\cite{kirkpatrick1983} is used with an initial temperature of 100. Each
iteration consists of 200 problem instances which contain between 2 and 50
agents, the number of agents forms a uniform distribution. For each drop in
temperature a new set of 200 problem instances is generated to avoid
overfitting to a certain set of problems. There is a time limit of 2000ms to
solve each instance. The results of the simulated
annealing process are shown in \autoref{tbl:annealing}. The path length weight
refers to the increase in path length that lower priority agents suffer when
they have to make a new length. Some proposals have the side effect of creating
or solving additional conflicts, this is weighted by the conflicts solved
weight. The partial solved weight is used in cases where multiple agents take
part in a dialogue and a proposal that only assigns priority to some of the
agents doesn't solve the conflict for all agents.

\begin{table}[t]
    \centering
    \caption{Evaluation weights as determined by simulated annealing. Empty
        fields mean that the weight is not used by that algorithm.}
    \label{tbl:annealing}
    \begin{tabular}{l|r|r|r}
        & Path length & Conflicts solved & Partial solution \\ \hline
        \Sexpr{cannonical.names[3]} & 4.743788 & 5.290992 &  \\
        \Sexpr{cannonical.names[4]} & 0.312151 & 5.569737 & 2.677335 \\
        \Sexpr{cannonical.names[5]} & 3.113396 & 9.46371 &  \\
        \Sexpr{cannonical.names[6]} & 8.735623 & 7.914287 &  \\
        \Sexpr{cannonical.names[7]} & 9.352366 & 22.87437 &
    \end{tabular}
\end{table}

\subsection{Experimental evaluation}
To compare the algorithms they are all given a large set of
cooperative pathfinding problems that have be to solve. Each instance
is constructed following the same procedure as used for simulated annealing.
Each instance has between 2 and \agentsupb, the number of agents still forms a
uniform distribution. The time it takes to solve each instance in the set of
problems is recorded. There is a time limit of 2000ms for each instance to
calculate a solution.

<<perfgraph,echo=F,warning=F,cache=T,cache.beater=dat,dev='tikz',fig.height=2.5,fig.width=5,fig.align='center',fig.pos='t',fig.cap=perfgraph.title>>=
op <- par(mar = c(3.8, 3.8, 1, 1))
plot(1,
    type='l',
    log='y',
    col='red',
    xlim=c(1,len),
    ylim=c(5, 2000),
    xlab='Problem instance',
    ylab='Time (ms)',
    frame.plot=F
)
lines(seq(length(od)), od, col='red')
lines(seq(length(naive)), naive, col='blue')
lines(seq(length(base)),  base,  col='green')
lines(seq(length(plus)),  plus,  col='grey')
lines(seq(length(window2)), window2, col='magenta')
lines(seq(length(window4)), window4, col='cyan')
lines(seq(length(window8)), window8, col='orange')
legend("bottomright",
    legend=cannonical.names,
    col=color.set,
    pch=16
)
@

% TODO: add reference to performance graph origin
When the run times are sorted in ascending order they can be plotted
in a \emph{performance graph} as in \autoref{fig:perfgraph}. The $x$-axis shown
the number of the sorted instance while the $y$-axis shows the time it takes an
algorithm to solve that instance. Because the instances are sorted it is not
necessarily the case that the $n$th instance for one algorithm is the same as
for a different algorithm. The graph gives several pieces of information. The
first is a comparison of run times for the different algorithms, a lower line
means that an algorithm was able to solve problem instances quicker. At the
same time the graph also shows how many instances an algorithm can solve within
the 2000 ms time limit. Instances that were not solved are not included in the
graph, so where the graph crosses the top indicates how many of the problems
were solved by the algorithm.

The graph in \autoref{fig:perfgraph} shows the performance characteristic for
the tested algorithms. It shows that only the naive approach is generally
slower than the complete algorithm OD+ID, in $\Sexpr{round(naive.vs.odid * 100,
1)}\%$ of the instances that
both algorithms solved OD+ID was faster. All other methods are faster than
OD+ID and are able to find a solution to more instances. The DPCA* and DPCA*+
versions of the algorithm have an almost equal performance. DPCA* is
slightly faster on each individual instance by
$\Sexpr{signif(abs(base.vs.plus$estimate * 1000), 2)}$ ms as shown by a
one-sided
paired t-test $(t(\Sexpr{base.vs.plus$parameter}) =
\Sexpr{round(base.vs.plus$statistic, 2)},
p=\Sexpr{signif(base.vs.plus$p.value, 3)})$. They are able to solve about the
same number of instances, DPCA* solves $\Sexpr{round(solved[3] * 100, 1)}\%$
of instances while DPCA*+ is able to solve $\Sexpr{round(solved[4] * 100,
1)}\%$.

The size of the window also has an influence on the performance. From
\autoref{fig:perfgraph} we can see that any restricted window size means that
the algorithm is faster and is able to solve more problem instances. We can
also see that the graphs for different window sizes are similar to each other.
An analysis of variance test shows that there is a difference in the time to
reach a solution for different window sizes $(F(\Sexpr{w.anova[['Df']][1]},
\Sexpr{w.anova[['Df']][3]}) = \Sexpr{round(w.anova[['F value']][1], 2)},
p=\Sexpr{signif(w.anova[['Pr(>F)']][1], 3)})$. Individual t-tests show that
$w=2$ differs from $w=8$ in run time ($t(\Sexpr{floor(t.2vs8$parameter)}) =
\Sexpr{round(t.2vs8$statistic, 2)}, p=\Sexpr{signif(t.2vs8$p.value, 3)}$).
There is no difference between $w=4$ and the other window sizes
($t(\Sexpr{floor(t.2vs4$parameter)})
= \Sexpr{round(t.2vs4$statistic, 2)}, p=\Sexpr{signif(t.2vs4$p.value, 3)}$ for
$w=2$ and $t(\Sexpr{floor(t.4vs8$parameter)}) = \Sexpr{round(t.4vs8$statistic,
2)}, p=\Sexpr{signif(t.4vs8$p.value, 3)}$ for $w=8$c).
%The window size has a large effect on the sum of the path lengths of the
%agents as shown in \autoref{tbl:length}.

<<quality,echo=F,cache=T,cache.beater=dat,results='asis'>>=
quality.table = t(rbind(paste(round(solved * 100, 1), "%"),
round(lengths$length, 2)))
colnames(quality.table) = c('Instances solved', 'Length')
rownames(quality.table) = cannonical.names
xtable(quality.table,
       label='tbl:quality',
       caption='Solution quality of algorithms. Length is the sum of the lengths
       of the paths for a single problem instance',
       align='l|r|r',
       digits=2
)
@

<<solved,echo=F,cache=T,cache.beater=solved.aggr,dev='tikz',fig.width=5,fig.height=2.5,fig.align='center',fig.pos='t',fig.cap=solved.title>>=
op <- par(mar = c(4, 3.8, 1, 1))
plot(1,
    type='l',
    xlim=c(0,40),
    ylim=c(0,1),
    xlab='Agents in problem instance',
    ylab='Percentage solved',
    frame.plot = F
)
for(i in 1:length(algorithms)) {
    lines(solved.aggr[solved.aggr[,1]==algorithms[i],]$time,
          col=color.set[i])
}
legend("bottomleft", legend=cannonical.names, col=color.set, pch=16)
@

As shown by \autoref{tbl:quality} \Sexpr{cannonical.names[2]} is the only
version of the algorithm that solves fewer instances than OD+ID. All versions
of \Sexpr{cannonical.names[3]} and WDPCA* are able to solve at least double the
amount of instances. A smaller window means that the algorithm is able to solve
more problem instances. At the same time this also comes at a cost, a smaller
window size results in longer sum path lengths as shown by
\autoref{tbl:quality}.

%\begin{figure}
%	\centering
%	\input{graphs/solved}
%	\caption{Percentage of problem instances solved}
%	\label{fig:solved}
%\end{figure}

%\begin{figure}
%	\centering
%	\input{graphs/lengths}
%	\caption{Lengths}
%	\label{fig:lengths}
%\end{figure}

The influence of caching paths that have been calculated is shown in
\autoref{fig:cache}. Only the effects on DPCA* and WDPCA* with $w=2$ and $w=8$
are shown to reduce clutter, the effects on DPCA*+ and WDPCA* with $w=4$ were
also tested. The dashed lines in the plot represent the
performance of an algorithm with caching enabled, while the dotted line shows
the performance with the cache disabled. It shows that using a cache has a
large effect on DPCA*, but not on WDPCA*-2. An analysis of variance shows that
caching does have an effect on the time to solve an instance
$(F(\Sexpr{cache.sum[['Df']][3]},
\Sexpr{cache.anova$df.residual}) = \Sexpr{round(cache.sum[['F value']][3], 2)},
p = \Sexpr{signif(cache.sum[['Pr(>F)']][3], 3)})$ while there
is also an interaction between the algorithm and whether the cache is enabled
$(F(\Sexpr{cache.sum[['Df']][4]}, \Sexpr{cache.anova$df.residual}) =
\Sexpr{round(cache.sum[['F value']][4], 2)},
p=\Sexpr{signif(cache.sum[['Pr(>F)']][3], 3)})$. This
interaction confirms that the effect of the cache does indeed depend on which
version of DPCA* is used.

<<cache,echo=F,warning=F,cache=F,cache.beater=dat.cache,dev='tikz',fig.width=5,fig.height=2.5,fig.align='center',fig.pos='t',fig.cap=cachegraph.title>>=
op <- par(mar = c(3.8, 3.8, 1, 1))
plot(c(), c(),
    type='l',
    log='y',
    col='red',
    xlim=c(1,len.cache),
    ylim=c(5, 2000),
    xlab='Instance',
    ylab='Time (ms)',
    frame.plot=F
)
lines(seq(length(base.nocache)), base.nocache, col=color.set.cache[1], lty=3)
#lines(seq(length(plus.nocache)), plus.nocache, col=color.set.cache[2], lty=3)
lines(seq(length(window2.nocache)), window2.nocache, col=color.set.cache[3],
    lty=3)
#lines(seq(length(window4.nocache)), window4.nocache, col=color.set.cache[4],
#	lty=3)
lines(seq(length(window8.nocache)), window8.nocache, col=color.set.cache[5],
    lty=3)
lines(seq(length(base.cache)), base.cache, col=color.set.cache[1], lty=2)
#lines(seq(length(plus.cache)), plus.cache, col=color.set.cache[2], lty=2)
lines(seq(length(window2.cache)), window2.cache, col=color.set.cache[3], lty=2)
#lines(seq(length(window4.cache)), window4.cache, col=color.set.cache[4], lty=2)
lines(seq(length(window8.cache)), window8.cache, col=color.set.cache[5], lty=2)
legend("bottomright",
    legend=cannonical.names.cache[c(1,3,5)],
    col=color.set.cache[c(1,3,5)],
    pch=16)
@
